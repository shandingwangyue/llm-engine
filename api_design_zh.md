# API æ¥å£è®¾è®¡è§„èŒƒ

## ğŸ¯ åŸºç¡€ä¿¡æ¯
- **åŸºç¡€URL**: `/api/v1` (è‡ªå®šä¹‰æ¥å£)
- **OpenAIå…¼å®¹URL**: `/v1` (å®Œå…¨å…¼å®¹OpenAI API)
- **è®¤è¯æ–¹å¼**: API Key (å¯é€‰) / JWT Token
- **å“åº”æ ¼å¼**: JSON
- **ç¼–ç **: UTF-8

## ğŸ“‹ æ ¸å¿ƒAPIç«¯ç‚¹

### 1. å¥åº·æ£€æŸ¥æ¥å£
```http
GET /health
```
**å“åº”**:
```json
{
  "status": "healthy",
  "model_loaded": true,
  "memory_usage": "2.1GB/8GB",
  "active_connections": 5
}
```

### 2. æ–‡æœ¬ç”Ÿæˆæ¥å£
```http
POST /generate
```
**è¯·æ±‚ä½“**:
```json
{
  "prompt": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ",
  "model": "llama-2-7b-chat",
  "max_tokens": 512,
  "temperature": 0.7,
  "stream": false
}
```

**å“åº”**:
```json
{
  "text": "äººå·¥æ™ºèƒ½æ˜¯...",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 128,
    "total_tokens": 143
  },
  "latency": 2.3
}
```

### 3. æµå¼ç”Ÿæˆæ¥å£
```http
POST /generate/stream
```
**è¯·æ±‚å¤´**:
```
Accept: text/event-stream
```

**è¯·æ±‚ä½“**:
```json
{
  "prompt": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ",
  "model": "qwen-7b-chat",
  "max_tokens": 512,
  "temperature": 0.7
}
```

**å“åº”** (SSEæ ¼å¼):
```
data: {"token": "äºº", "finished": false}

data: {"token": "å·¥", "finished": false}

data: {"token": "æ™º", "finished": false}

data: {"token": "èƒ½", "finished": true}
```

### 4. æ¨¡å‹ç®¡ç†æ¥å£
```http
GET /models
```
**å“åº”**:
```json
{
  "models": [
    {
      "name": "llama-2-7b-chat",
      "format": "gguf",
      "size": "4.2GB",
      "loaded": true,
      "memory_usage": "3.8GB"
    }
  ]
}
```

```http
POST /models/{model_name}/load
```
**è¯·æ±‚ä½“**:
```json
{
  "quantization": "q4_0",
  "prefer_gpu": false
}
```

## ğŸ”§ é”™è¯¯å¤„ç†

### é”™è¯¯å“åº”æ ¼å¼
```json
{
  "error": {
    "code": "MODEL_NOT_LOADED",
    "message": "è¯·æ±‚çš„æ¨¡å‹æœªåŠ è½½",
    "details": "è¯·å…ˆè°ƒç”¨åŠ è½½æ¥å£"
  }
}
```

### å¸¸è§é”™è¯¯ç 
- `400`: è¯·æ±‚å‚æ•°é”™è¯¯
- `401`: è®¤è¯å¤±è´¥
- `404`: èµ„æºæœªæ‰¾åˆ°
- `429`: è¯·æ±‚è¿‡äºé¢‘ç¹
- `500`: æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
- `503`: æœåŠ¡ä¸å¯ç”¨

## ğŸ“Š æ€§èƒ½ç›‘æ§æ¥å£

```http
GET /metrics
```
**å“åº”** (Prometheusæ ¼å¼):
```
# HELP model_inference_seconds æ¨¡å‹æ¨ç†è€—æ—¶
# TYPE model_inference_seconds histogram
model_inference_seconds_bucket{le="1"} 15
model_inference_seconds_bucket{le="3"} 42
model_inference_seconds_bucket{le="5"} 78

# HELP active_connections æ´»è·ƒè¿æ¥æ•°
# TYPE active_connections gauge
active_connections 23
```

## ğŸ›¡ï¸ é™æµç­–ç•¥

### è¯·æ±‚å¤´ä¿¡æ¯
```
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1633046400
```

## ğŸ”„ æ‰¹é‡è¯·æ±‚æ”¯æŒ

```http
POST /batch/generate
```
**è¯·æ±‚ä½“**:
```json
{
  "requests": [
    {
      "prompt": "é—®é¢˜1",
      "max_tokens": 100
    },
    {
      "prompt": "é—®é¢˜2", 
      "max_tokens": 150
    }
  ]
}
```

**å“åº”**:
```json
{
  "results": [
    {
      "text": "å›ç­”1",
      "usage": {...}
    },
    {
      "text": "å›ç­”2",
      "usage": {...}
    }
  ]
}
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Pythonå®¢æˆ·ç«¯ç¤ºä¾‹
```python
import requests

def generate_text(prompt, model="default"):
    url = "http://localhost:8000/api/v1/generate"
    payload = {
        "prompt": prompt,
        "model": model,
        "max_tokens": 512,
        "temperature": 0.7
    }
    
    response = requests.post(url, json=payload)
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = generate_text("è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")
print(result["text"])
```

### JavaScriptå®¢æˆ·ç«¯ç¤ºä¾‹
```javascript
async function streamGenerate(prompt) {
    const response = await fetch('/api/v1/generate/stream', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Accept': 'text/event-stream'
        },
        body: JSON.stringify({ prompt })
    });
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
        const { value, done } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const data = JSON.parse(chunk.replace('data: ', ''));
        console.log(data.token);
    }
}

## ğŸŒ OpenAIå…¼å®¹æ¥å£

### Chat Completionsæ¥å£
```http
POST /v1/chat/completions
```
**è¯·æ±‚ä½“** (éæµå¼):
```json
{
  "model": "qwen-7b-chat",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ"}
  ],
  "max_tokens": 512,
  "temperature": 0.7,
  "stream": false
}
```

**è¯·æ±‚ä½“** (æµå¼):
```json
{
  "model": "qwen-7b-chat",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ"}
  ],
  "max_tokens": 512,
  "temperature": 0.7,
  "stream": true  // å¯ç”¨æµå¼è¾“å‡º
}
```

**æµå¼å“åº”** (SSEæ ¼å¼):
```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"äºº"},"index":0}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"å·¥"},"index":0}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"æ™º"},"index":0}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"èƒ½"},"index":0}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{},"finish_reason":"stop","index":0}]}

data: [DONE]
```

**å“åº”ä½“**:
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "qwen-7b-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "äººå·¥æ™ºèƒ½æ˜¯..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 128,
    "total_tokens": 143
  }
}
```

### Modelsåˆ—è¡¨æ¥å£
```http
GET /v1/models
```

### æ”¯æŒçš„ä¸»æµæ¨¡å‹
- `qwen-7b-chat` - é˜¿é‡Œé€šä¹‰åƒé—®7BèŠå¤©æ¨¡å‹
- `qwen-14b-chat` - é˜¿é‡Œé€šä¹‰åƒé—®14BèŠå¤©æ¨¡å‹
- `gemma-7b` - Google Gemma 7Bæ¨¡å‹
- `gemma-2b` - Google Gemma 2Bæ¨¡å‹
- `llama-2-7b-chat` - Meta LLaMA 2 7BèŠå¤©æ¨¡å‹
- `chatglm3-6b` - æ¸…åæ™ºè°±ChatGLM3 6Bæ¨¡å‹

### ä½¿ç”¨å®˜æ–¹OpenAI SDKç¤ºä¾‹
```python
import openai

# é…ç½®æŒ‡å‘æœ¬åœ°æœåŠ¡
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "any-key"  # å¯é€‰çš„è®¤è¯

# å®Œå…¨å…¼å®¹OpenAI API
response = openai.ChatCompletion.create(
    model="qwen-7b-chat",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ"}
    ]
)

print(response.choices[0].message.content)
```

### LangChainé›†æˆç¤ºä¾‹
```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
llm = OpenAI(
    openai_api_base="http://localhost:8000/v1",
    model_name="qwen-7b-chat"
)

# æˆ–è€…ä½¿ç”¨èŠå¤©æ¨¡å‹
chat = ChatOpenAI(
    openai_api_base="http://localhost:8000/v1",
    model_name="qwen-7b-chat"
)
```