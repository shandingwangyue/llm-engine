# OpenAIå…¼å®¹æ¥å£è®¾è®¡

## ğŸ¯ OpenAI APIå…¼å®¹æ€§

### 1. æ ¸å¿ƒå…¼å®¹æ¥å£

#### Chat Completionsæ¥å£ (OpenAIæ ‡å‡†)
```http
POST /v1/chat/completions
```
**è¯·æ±‚ä½“** (OpenAIå…¼å®¹æ ¼å¼):
```json
{
  "model": "qwen-7b-chat",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ"}
  ],
  "max_tokens": 512,
  "temperature": 0.7,
  "stream": false
}
```

**å“åº”ä½“** (OpenAIå…¼å®¹æ ¼å¼):
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "qwen-7b-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "äººå·¥æ™ºèƒ½æ˜¯..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 128,
    "total_tokens": 143
  }
}
```

#### Completionsæ¥å£ (ä¼ ç»Ÿæ ¼å¼)
```http
POST /v1/completions
```
**è¯·æ±‚ä½“**:
```json
{
  "model": "gemma-7b",
  "prompt": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ",
  "max_tokens": 512,
  "temperature": 0.7
}
```

### 2. æ¨¡å‹å‘ç°æ¥å£
```http
GET /v1/models
```
**å“åº”ä½“**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen-7b-chat",
      "object": "model",
      "created": 1677616000,
      "owned_by": "local",
      "permission": [...],
      "root": "qwen-7b-chat",
      "parent": null
    },
    {
      "id": "gemma-7b",
      "object": "model", 
      "created": 1677616000,
      "owned_by": "local",
      "permission": [...],
      "root": "gemma-7b",
      "parent": null
    }
  ]
}
```

## ğŸ”§ Qwenæ¨¡å‹æ”¯æŒ

### æ¨¡å‹é…ç½®
```python
# Qwenæ¨¡å‹ä¸“ç”¨é…ç½®
QWEN_MODEL_CONFIG = {
    "qwen-7b": {
        "format": "gguf",
        "recommended_quantization": "q4_0",
        "context_length": 4096,
        "special_tokens": {
            "bos_token": "<|im_start|>",
            "eos_token": "<|im_end|>",
            "pad_token": "<|extra_0|>"
        }
    },
    "qwen-14b": {
        "format": "gguf", 
        "recommended_quantization": "q4_0",
        "context_length": 4096
    }
}
```

### Qwenå¯¹è¯æ ¼å¼å¤„ç†
```python
def format_qwen_chat(messages):
    """å°†OpenAIæ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸ºQwenå¯¹è¯æ ¼å¼"""
    formatted = ""
    for msg in messages:
        if msg["role"] == "system":
            formatted += f"<|im_start|>system\n{msg['content']}<|im_end|>\n"
        elif msg["role"] == "user":
            formatted += f"<|im_start|>user\n{msg['content']}<|im_end|>\n"
        elif msg["role"] == "assistant":
            formatted += f"<|im_start|>assistant\n{msg['content']}<|im_end|>\n"
    
    formatted += "<|im_start|>assistant\n"
    return formatted
```

## ğŸ”§ Gemmaæ¨¡å‹æ”¯æŒ

### æ¨¡å‹é…ç½®
```python
# Gemmaæ¨¡å‹ä¸“ç”¨é…ç½®
GEMMA_MODEL_CONFIG = {
    "gemma-7b": {
        "format": "gguf",
        "recommended_quantization": "q4_0", 
        "context_length": 8192,
        "special_tokens": {
            "bos_token": "<bos>",
            "eos_token": "<eos>",
            "pad_token": "<pad>"
        }
    },
    "gemma-2b": {
        "format": "gguf",
        "recommended_quantization": "q4_0",
        "context_length": 8192
    }
}
```

### Gemmaå¯¹è¯æ ¼å¼å¤„ç†
```python
def format_gemma_chat(messages):
    """å°†OpenAIæ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸ºGemmaå¯¹è¯æ ¼å¼"""
    conversation = []
    for msg in messages:
        if msg["role"] == "system":
            conversation.append(f"<start_of_turn>system\n{msg['content']}<end_of_turn>")
        elif msg["role"] == "user":
            conversation.append(f"<start_of_turn>user\n{msg['content']}<end_of_turn>")
        elif msg["role"] == "assistant":
            conversation.append(f"<start_of_turn>model\n{msg['content']}<end_of_turn>")
    
    conversation.append("<start_of_turn>model\n")
    return "\n".join(conversation)
```

## ğŸ—ï¸ æ‰©å±•æ¨¡å‹åŠ è½½å™¨

### æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹
```python
# app/core/model_loader.py - æ‰©å±•ç‰ˆæœ¬

class ExtendedModelLoader(ModelLoader):
    def __init__(self):
        super().__init__()
        self.supported_formats.extend(['.safetensors', '.pt'])
        
        # æ¨¡å‹ç‰¹å®šé…ç½®
        self.model_configs = {
            'qwen': QWEN_MODEL_CONFIG,
            'gemma': GEMMA_MODEL_CONFIG,
            'llama': LLAMA_MODEL_CONFIG,
            'chatglm': CHATGLM_MODEL_CONFIG
        }
    
    def detect_model_type(self, model_path: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹"""
        model_name = os.path.basename(model_path).lower()
        
        if 'qwen' in model_name:
            return 'qwen'
        elif 'gemma' in model_name:
            return 'gemma' 
        elif 'chatglm' in model_name:
            return 'chatglm'
        elif 'llama' in model_name:
            return 'llama'
        else:
            return 'unknown'
    
    def load_model_with_config(self, model_path: str, **kwargs):
        """æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½å¹¶åº”ç”¨ç‰¹å®šé…ç½®"""
        model_type = self.detect_model_type(model_path)
        
        if model_type in self.model_configs:
            model_config = self.model_configs[model_type]
            # åº”ç”¨æ¨¡å‹ç‰¹å®šé…ç½®
            kwargs.update(model_config.get('default_params', {}))
        
        return self.load_model(model_path, **kwargs)
```

## ğŸ“‹ OpenAIå…¼å®¹è·¯ç”±å®ç°

```python
# app/routers/openai.py
from fastapi import APIRouter, HTTPException
from typing import List, Optional
from pydantic import BaseModel
import time
import shortuuid

router = APIRouter(prefix="/v1", tags=["openai"])

# OpenAIå…¼å®¹çš„æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: Optional[int] = None
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = False

class CompletionRequest(BaseModel):
    model: str
    prompt: str
    max_tokens: Optional[int] = None
    temperature: Optional[float] = 0.7

@router.post("/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAIå…¼å®¹çš„èŠå¤©è¡¥å…¨æ¥å£"""
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if request.model.startswith('qwen'):
            prompt = format_qwen_chat(request.messages)
        elif request.model.startswith('gemma'):
            prompt = format_gemma_chat(request.messages)
        else:
            # é»˜è®¤æ ¼å¼
            prompt = "\n".join([f"{msg.role}: {msg.content}" for msg in request.messages])
        
        # æ‰§è¡Œæ¨ç†
        result = await inference_service.generate_text(
            request.model, prompt, 
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        # æ„å»ºOpenAIå…¼å®¹å“åº”
        response = {
            "id": f"chatcmpl-{shortuuid.uuid()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["text"]
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": result["usage"]
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/completions") 
async def completions(request: CompletionRequest):
    """OpenAIå…¼å®¹çš„è¡¥å…¨æ¥å£"""
    try:
        result = await inference_service.generate_text(
            request.model, request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        response = {
            "id": f"cmpl-{shortuuid.uuid()}",
            "object": "text_completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "text": result["text"],
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": "length"
                }
            ],
            "usage": result["usage"]
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models")
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    models = model_manager.list_models()
    
    model_list = []
    for model_name, model_info in models.items():
        model_list.append({
            "id": model_name,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "local",
            "permission": [
                {
                    "id": f"modelperm-{shortuuid.uuid()}",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ],
            "root": model_name,
            "parent": None
        })
    
    return {"object": "list", "data": model_list}
```

## ğŸš€ å®¢æˆ·ç«¯å…¼å®¹æ€§

### OpenAI SDKç›´æ¥ä½¿ç”¨
```python
# ä½¿ç”¨å®˜æ–¹OpenAI SDK
import openai

# é…ç½®æŒ‡å‘æœ¬åœ°æœåŠ¡
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "your-api-key"  # å¯é€‰çš„è®¤è¯

# ç›´æ¥è°ƒç”¨ï¼ˆä¸OpenAI APIå®Œå…¨å…¼å®¹ï¼‰
response = openai.ChatCompletion.create(
    model="qwen-7b-chat",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¦‚å¿µ"}
    ]
)

print(response.choices[0].message.content)
```

### LangChainé›†æˆ
```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
llm = OpenAI(
    openai_api_base="http://localhost:8000/v1",
    model_name="qwen-7b-chat"
)

# æˆ–è€…ä½¿ç”¨èŠå¤©æ¨¡å‹
chat = ChatOpenAI(
    openai_api_base="http://localhost:8000/v1",
    model_name="qwen-7b-chat"
)
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹å®¶æ— | æ”¯æŒç‰ˆæœ¬ | æ¨èé‡åŒ– | ç‰¹ç‚¹ |
|----------|----------|----------|------|
| **Qwen** | 7B, 14B | q4_0 | é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡ä¼˜åŒ– |
| **Gemma** | 2B, 7B | q4_0 | Googleè½»é‡çº§æ¨¡å‹ |
| **LLaMA** | 7B, 13B | q4_0 | Metaå¼€æºæ¨¡å‹ |
| **ChatGLM** | 6B | q4_0 | æ¸…åæ™ºè°±AI |
| **å…¶ä»–** | è‡ªå®šä¹‰ | q4_0 | æ”¯æŒä»»æ„GGUFæ ¼å¼ |

è¿™ä¸ªæ‰©å±•è®¾è®¡ç¡®ä¿äº†ä¸OpenAI APIçš„å®Œå…¨å…¼å®¹æ€§ï¼Œå¹¶å¢åŠ äº†å¯¹Qwenå’ŒGemmaæ¨¡å‹çš„ä¸“é—¨æ”¯æŒã€‚