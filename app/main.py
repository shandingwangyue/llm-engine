"""
å¤§æ¨¡å‹æœåŠ¡å¼•æ“ä¸»åº”ç”¨å…¥å£
"""

import logging
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.config import settings
from app.core.model_manager import init_model_manager
from app.core.async_inference import async_inference_service
from app.routers import health, generate, models, openai, memory

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, settings.log_level.upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLM Service Engine",
    description="é«˜æ€§èƒ½å¤§æ¨¡å‹æœ¬åœ°æœåŠ¡å¼•æ“ï¼Œæ”¯æŒå¤šç§å¼€æºå¤§æ¨¡å‹å’ŒOpenAIå…¼å®¹API",
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(health.router, prefix="/api/v1", tags=["health"])
app.include_router(generate.router, prefix="/api/v1", tags=["generate"])
app.include_router(models.router, prefix="/api/v1", tags=["models"])
app.include_router(memory.router, prefix="/api/v1", tags=["memory"])
app.include_router(openai.router, prefix="/v1", tags=["openai"])

# æ·»åŠ å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ å¤§æ¨¡å‹æœåŠ¡å¼•æ“å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ“Š é…ç½®ä¿¡æ¯: host={settings.host}, port={settings.port}")
    logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•: {settings.model_dir}")
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    try:
        init_model_manager()
        logger.info("âœ… æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # å¯åŠ¨å¼‚æ­¥æ¨ç†æœåŠ¡å¤„ç†ä»»åŠ¡ï¼ˆåœ¨äº‹ä»¶å¾ªç¯è¿è¡Œåï¼‰
        await async_inference_service.start_processing()
        logger.info("âœ… å¼‚æ­¥æ¨ç†æœåŠ¡å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info("ğŸ›‘ å¤§æ¨¡å‹æœåŠ¡å¼•æ“å…³é—­ä¸­...")
    # æ¸…ç†å¼‚æ­¥æ¨ç†æœåŠ¡
    await async_inference_service.shutdown()
    # å…¶ä»–æ¸…ç†å·¥ä½œä¼šåœ¨atexitä¸­è‡ªåŠ¨å¤„ç†

# æ ¹è·¯å¾„
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ¬¢è¿ä½¿ç”¨å¤§æ¨¡å‹æœåŠ¡å¼•æ“",
        "version": "0.1.0",
        "docs": "/docs",
        "openai_api": "/v1"
    }

# å¥åº·æ£€æŸ¥ï¼ˆå…¼å®¹æ€§ï¼‰
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£ï¼ˆå…¼å®¹æ€§ï¼‰"""
    return {"status": "healthy"}

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œåº”ç”¨
    uvicorn.run(
        app,
        host=settings.host,
        port=settings.port,
        workers=settings.workers,
        log_level=settings.log_level
    )